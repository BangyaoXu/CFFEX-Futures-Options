# -*- coding: utf-8 -*-
"""
CFFEX Futures & Options Dashboard (Streamlit)
============================================================================

Install:
  pip install streamlit pandas numpy scipy plotly beautifulsoup4 lxml requests

Run:
  streamlit run cffex_app.py

Place your CFFEX html/mhtml files in the same folder as cffex_app.py
(or point the app to another folder in the sidebar).
"""

from __future__ import annotations

import re
import math
import json
import datetime as dt
from pathlib import Path
from email import message_from_binary_file
from typing import List, Optional, Dict, Tuple
from io import StringIO

import numpy as np
import pandas as pd
import streamlit as st
import plotly.express as px
from scipy.optimize import brentq
from bs4 import BeautifulSoup
import requests


# =========================
# USER MAPPING (ENFORCED)
# =========================
FUTURES_NAME_MAP = {
    "IF": "æ²ªæ·±300è‚¡æŒ‡æœŸè´§",
    "IM": "ä¸­è¯1000è‚¡æŒ‡æœŸè´§",
    "IH": "ä¸Šè¯50è‚¡æŒ‡æœŸè´§",
}
OPTIONS_NAME_MAP = {
    "IO": "æ²ªæ·±300è‚¡æŒ‡æœŸæƒ",
    "MO": "ä¸­è¯1000è‚¡æŒ‡æœŸæƒ",
    "HO": "ä¸Šè¯50è‚¡æŒ‡æœŸæƒ",
}
FUT_PREFIXES = list(FUTURES_NAME_MAP.keys())
OPT_PREFIXES = list(OPTIONS_NAME_MAP.keys())

OPT_TO_FUT = {"IO": "IF", "HO": "IH", "MO": "IM"}

OPTIONS_NAME_EN = {
    "IO": "CSI 300 Index Options",
    "MO": "CSI 1000 Index Options",
    "HO": "SSE 50 Index Options",
}
FUTURES_NAME_EN = {
    "IF": "CSI 300 Index Futures",
    "IM": "CSI 1000 Index Futures",
    "IH": "SSE 50 Index Futures",
}

# DISPLAY ONLY (your tradable ETF)
ETF_MAP = {
    "IO": {"name_cn": "æ²ªæ·±300 ETF", "ticker": "510300.SH"},
    "MO": {"name_cn": "ä¸­è¯1000 ETF", "ticker": "159845.SZ"},
    "HO": {"name_cn": "ä¸Šè¯50 ETF", "ticker": "510050.SH"},
}

# âœ… USED for basis/carry: Yahoo Finance index symbols (as you requested)
INDEX_MAP = {
    "IF": {"name_cn": "æ²ªæ·±300æŒ‡æ•°", "ticker": "000300.SS"},
    "IH": {"name_cn": "ä¸Šè¯50æŒ‡æ•°", "ticker": "000016.SS"},
    "IM": {"name_cn": "ä¸­è¯1000æŒ‡æ•°", "ticker": "399852.SZ"},
}


# =========================
# Utilities
# =========================
def _num(x) -> Optional[float]:
    if x is None:
        return None
    s = str(x).strip().replace(",", "")
    if s in ("", "-", "â€”", "â€“", "NaN", "nan", "None"):
        return None
    s = re.sub(r"[^0-9\.\-\+]", "", s)
    try:
        return float(s)
    except Exception:
        return None


def third_friday(year: int, month: int) -> dt.date:
    first = dt.date(year, month, 1)
    off = (4 - first.weekday()) % 7  # Friday=4
    return first + dt.timedelta(days=off + 14)


def infer_expiry_from_yymm(yymm: str) -> Optional[dt.date]:
    try:
        yy, mm = int(yymm[:2]), int(yymm[2:])
        return third_friday(2000 + yy, mm)
    except Exception:
        return None


def infer_expiry(sym: str) -> Optional[dt.date]:
    s = sym.upper().strip()
    m = re.search(r"(IF|IH|IM)(\d{4})", s)
    if m:
        return infer_expiry_from_yymm(m.group(2))
    m2 = re.search(r"(IO|HO|MO)(\d{4})", s)
    if m2:
        return infer_expiry_from_yymm(m2.group(2))
    return None


def yearfrac(today: dt.date, expiry: dt.date) -> float:
    return max((expiry - today).days, 0) / 365.0


def _find_col(cols: List[str], key: str) -> Optional[str]:
    return next((c for c in cols if key in c), None)


def pick_adjacent_or_nearest_latest(cols: List[str], strike_idx: int) -> Tuple[Optional[str], Optional[str]]:
    """
    CFFEX paired option table:
      CallPx = column immediately LEFT of è¡Œæƒä»· (prefer æœ€æ–°ä»·)
      PutPx  = column immediately RIGHT of è¡Œæƒä»· (prefer æœ€æ–°ä»·)
    Fallback to nearest æœ€æ–°ä»· on each side if adjacency is not æœ€æ–°ä»·.
    """
    call_px_col = None
    put_px_col = None

    if strike_idx - 1 >= 0 and "æœ€æ–°ä»·" in cols[strike_idx - 1]:
        call_px_col = cols[strike_idx - 1]
    if strike_idx + 1 < len(cols) and "æœ€æ–°ä»·" in cols[strike_idx + 1]:
        put_px_col = cols[strike_idx + 1]

    if call_px_col is None:
        for j in range(strike_idx - 1, -1, -1):
            if "æœ€æ–°ä»·" in cols[j]:
                call_px_col = cols[j]
                break

    if put_px_col is None:
        for j in range(strike_idx + 1, len(cols)):
            if "æœ€æ–°ä»·" in cols[j]:
                put_px_col = cols[j]
                break

    return call_px_col, put_px_col


def _score_header_row(row: List[str], want: str) -> int:
    r = [str(x).strip() for x in row]
    s = 0
    if want == "options":
        s += 50 if any("è¡Œæƒä»·" in x for x in r) else 0
        s += 30 if sum(("æœ€æ–°ä»·" in x) for x in r) >= 2 else 0
        s += 10 if any("çœ‹æ¶¨" in x for x in r) else 0
        s += 10 if any("çœ‹è·Œ" in x for x in r) else 0
        s += 5 if any("æˆäº¤é‡" in x for x in r) else 0
        s += 5 if any("æŒä»“é‡" in x for x in r) else 0
        s += 2 if any("æ¶¨è·Œ" in x for x in r) else 0
    else:
        s += 40 if any("åˆçº¦åç§°" in x for x in r) else 0
        s += 20 if any("æœ€æ–°ä»·" in x for x in r) else 0
        s += 10 if any("æ”¶ç›˜" in x for x in r) else 0
        s += 10 if any("ç»“ç®—" in x for x in r) else 0
        s += 5 if any("æˆäº¤é‡" in x for x in r) else 0
        s += 5 if any("æŒä»“é‡" in x for x in r) else 0
    return s


def _make_unique_cols(cols: List[str]) -> List[str]:
    out = []
    for i, c in enumerate(cols):
        name = str(c).strip()
        out.append(f"{name}__{i}" if name else f"__blank__{i}")
    return out


def _as_float(x) -> Optional[float]:
    try:
        if x is None:
            return None
        v = float(x)
        if np.isnan(v):
            return None
        return v
    except Exception:
        return None


def _pick_front_expiry(df: pd.DataFrame, col: str = "expiry") -> Optional[dt.date]:
    if df is None or df.empty:
        return None
    xs = sorted(df[col].dropna().unique())
    return xs[0] if xs else None


def _sigmoid_to_0_100(x: float) -> int:
    y = 1.0 / (1.0 + math.exp(-x))
    return int(round(100.0 * y))


def _surface_horizon_from_points(surf_points: Optional[pd.DataFrame]) -> str:
    if surf_points is None or surf_points.empty:
        return "3â€“10 trading days"

    hi = surf_points.dropna(subset=["iv", "expiry"]).sort_values("iv", ascending=False).head(30)
    if hi.empty:
        return "3â€“10 trading days"

    e_mode = hi["expiry"].mode()
    e_star = e_mode.iloc[0] if len(e_mode) else None
    if e_star is None:
        return "3â€“10 trading days"

    expiries_sorted = sorted(surf_points["expiry"].dropna().unique())
    if not expiries_sorted:
        return "3â€“10 trading days"

    front = expiries_sorted[0]
    if e_star == front:
        return "1â€“5 trading days (event / front-expiry dominated)"
    if len(expiries_sorted) >= 2 and e_star == expiries_sorted[1]:
        return "3â€“10 trading days"
    return "2â€“6 weeks (mid/long-expiry dominated)"


def _unit_sanity_ok(F: Optional[float], S: Optional[float]) -> bool:
    """Guard against ETF (~3) vs index (~3000) mismatch."""
    if F is None or S is None:
        return False
    if not np.isfinite(F) or not np.isfinite(S):
        return False
    if F <= 0 or S <= 0:
        return False
    ratio = F / S
    return 0.5 <= ratio <= 2.0


# =========================
# âœ… Yahoo Finance Index Spot Fetch
# =========================
CN_TZ = dt.timezone(dt.timedelta(hours=8))

@st.cache_data(ttl=60, show_spinner=False)
def yahoo_last_price(symbol: str) -> Tuple[Optional[float], Optional[dt.datetime], str]:
    """
    Fetch last price from Yahoo Finance chart API.
    Returns (price, timestamp_dt, note).
    """
    url = f"https://query1.finance.yahoo.com/v8/finance/chart/{symbol}"
    params = {"interval": "1m", "range": "1d"}  # intraday last
    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json,text/plain,*/*",
        "Accept-Language": "en-US,en;q=0.9",
        "Connection": "keep-alive",
    }
    try:
        r = requests.get(url, params=params, headers=headers, timeout=8)
        if r.status_code != 200:
            return None, None, f"HTTP {r.status_code}"
        j = r.json()

        chart = j.get("chart", {})
        if chart.get("error"):
            return None, None, f"chart.error: {chart['error']}"
        res = (chart.get("result") or [])
        if not res:
            return None, None, "no result"
        res0 = res[0]

        meta = res0.get("meta", {})
        px = meta.get("regularMarketPrice")
        ts = meta.get("regularMarketTime")

        px = _as_float(px)
        if px is None:
            return None, None, "no regularMarketPrice"

        ts_dt = None
        if ts is not None:
            try:
                ts_dt = dt.datetime.fromtimestamp(int(ts), tz=dt.timezone.utc).astimezone(CN_TZ)
            except Exception:
                ts_dt = None

        return px, ts_dt, "ok"
    except Exception as e:
        return None, None, f"exception: {type(e).__name__}: {e}"


def fetch_index_spot_price(index_ticker: str) -> Tuple[Optional[float], Optional[dt.datetime], str]:
    """
    Wrapper: returns (spot_px, ts, note).
    """
    return yahoo_last_price(index_ticker)


# =========================
# MHTML/HTML table extraction
# =========================
def extract_tables_from_mhtml_or_html(path: Path) -> List[pd.DataFrame]:
    html_parts: List[str] = []
    suffix = path.suffix.lower()

    if suffix == ".mhtml":
        with open(path, "rb") as f:
            msg = message_from_binary_file(f)
        for part in msg.walk():
            if part.get_content_type() == "text/html":
                payload = part.get_payload(decode=True)
                if payload:
                    html_parts.append(payload.decode(errors="ignore"))
    else:
        html_parts.append(path.read_text(errors="ignore"))

    tables_all: List[pd.DataFrame] = []

    for html in html_parts:
        # 1) Prefer pandas.read_html (handles colspan/rowspan)
        try:
            dfs = pd.read_html(StringIO(html))
            for df0 in dfs:
                if df0 is None or df0.empty:
                    continue
                df0 = df0.copy()
                df0.columns = _make_unique_cols([str(c) for c in df0.columns])

                def _row_keep(sr: pd.Series) -> bool:
                    vals = ["" if pd.isna(x) else str(x).strip() for x in sr.values]
                    has_num = any(_num(x) is not None for x in vals)
                    has_key = any(any(k in v for k in ["è¡Œæƒä»·", "æœ€æ–°ä»·", "åˆçº¦åç§°", "çœ‹æ¶¨", "çœ‹è·Œ"]) for v in vals)
                    return has_num or has_key

                df0 = df0[df0.apply(_row_keep, axis=1)].reset_index(drop=True)
                if len(df0) >= 3:
                    tables_all.append(df0)

            if tables_all:
                continue
        except Exception:
            pass

        # 2) Fallback: BeautifulSoup + header-row detect
        soup = BeautifulSoup(html, "lxml")
        for table in soup.find_all("table"):
            rows = []
            for tr in table.find_all("tr"):
                cells = [td.get_text(strip=True) for td in tr.find_all(["td", "th"])]
                if cells:
                    rows.append(cells)
            if len(rows) <= 2:
                continue

            maxc = max(len(r) for r in rows)
            padded = [r + [""] * (maxc - len(r)) for r in rows]

            N = min(4, len(padded))
            scored = []
            for i in range(N):
                scored.append((max(_score_header_row(padded[i], "options"), _score_header_row(padded[i], "futures")), i))
            scored.sort(reverse=True)
            header_idx = scored[0][1]

            header = padded[header_idx]
            data = padded[header_idx + 1 :]

            cleaned = []
            for r0 in data:
                has_num = any(_num(x) is not None for x in r0)
                keep_marker = (not has_num) and (any("çœ‹æ¶¨" in str(x) for x in r0) or any("çœ‹è·Œ" in str(x) for x in r0))
                if has_num or keep_marker:
                    cleaned.append(r0)

            if len(cleaned) < 2:
                continue

            df = pd.DataFrame(cleaned, columns=_make_unique_cols(header))
            tables_all.append(df)

    return tables_all


def pick_best_table(tables: List[pd.DataFrame], pattern: str, want: str) -> Optional[pd.DataFrame]:
    best, best_score = None, -1
    pat = re.compile(pattern, re.I)

    for df in tables:
        if df is None or df.empty:
            continue

        flat = df.astype(str).stack()
        cols = list(df.columns)

        score = 0
        if flat.str.contains(pat).any():
            score += 10

        if want == "futures":
            if any("åˆçº¦åç§°" in c for c in cols):
                score += 6
            if any(k in " ".join(cols) for k in ["æœ€æ–°ä»·", "æ”¶ç›˜", "ç»“ç®—"]):
                score += 4
        else:
            if any("è¡Œæƒä»·" in c for c in cols):
                score += 14
            if sum(("æœ€æ–°ä»·" in c) for c in cols) >= 2:
                score += 12
            if any("çœ‹æ¶¨" in c for c in cols) or flat.str.contains("çœ‹æ¶¨").any():
                score += 2
            if any("çœ‹è·Œ" in c for c in cols) or flat.str.contains("çœ‹è·Œ").any():
                score += 2

        if df.shape[0] < 3:
            score -= 5

        if score > best_score:
            best, best_score = df, score

    return best


# =========================
# Futures parsing
# =========================
FUT_COLS = ["product", "product_name", "symbol", "expiry", "last", "volume", "oi"]

def parse_futures(df: Optional[pd.DataFrame], prefix: str) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame(columns=FUT_COLS)

    cols = list(df.columns)
    sym_col = _find_col(cols, "åˆçº¦åç§°") or cols[0]

    last_col = (
        _find_col(cols, "æœ€æ–°ä»·")
        or _find_col(cols, "æ”¶ç›˜ä»·")
        or _find_col(cols, "ç»“ç®—ä»·")
        or _find_col(cols, "æ”¶ç›˜")
        or _find_col(cols, "ç»“ç®—")
        or (cols[1] if len(cols) > 1 else cols[0])
    )

    vol_col = _find_col(cols, "æˆäº¤é‡")
    oi_col = _find_col(cols, "æŒä»“é‡")

    out = []
    for _, r0 in df.iterrows():
        sym = str(r0.get(sym_col, "")).strip().upper()
        if not re.match(rf"^{prefix}\d{{4}}$", sym):
            continue
        expiry = infer_expiry(sym)
        last = _num(r0.get(last_col))
        if expiry is None or last is None:
            continue
        out.append(
            {
                "product": prefix,
                "product_name": FUTURES_NAME_MAP[prefix],
                "symbol": sym,
                "expiry": expiry,
                "last": float(last),
                "volume": _num(r0.get(vol_col)) if vol_col else None,
                "oi": _num(r0.get(oi_col)) if oi_col else None,
            }
        )

    if not out:
        return pd.DataFrame(columns=FUT_COLS)

    return pd.DataFrame(out).sort_values("expiry").reset_index(drop=True)


# =========================
# Options parsing (MULTI-EXPIRY WITHIN ONE TABLE)
# =========================
OPT_COLS = ["product", "product_name", "expiry", "cp", "K", "price", "volume", "oi", "symbol"]

def _extract_yymm_marker_from_row(values: List[str], prefix: str) -> Optional[str]:
    joined = " ".join(values).upper()
    if ("çœ‹æ¶¨" in joined) and ("çœ‹è·Œ" in joined):
        m = re.search(rf"{prefix}(\d{{4}})", joined, re.I)
        if m:
            return m.group(1)
    return None


def parse_options_paired_cn_mode(df: pd.DataFrame, prefix: str) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame(columns=OPT_COLS)

    cols = list(df.columns)
    strike_col = _find_col(cols, "è¡Œæƒä»·")
    if strike_col is None:
        return pd.DataFrame(columns=OPT_COLS)

    strike_idx = cols.index(strike_col)
    call_price_col, put_price_col = pick_adjacent_or_nearest_latest(cols, strike_idx)
    if call_price_col is None or put_price_col is None:
        return pd.DataFrame(columns=OPT_COLS)

    call_side_cols = cols[:strike_idx]
    put_side_cols = cols[strike_idx + 1 :]

    call_vol_col = next((c for c in call_side_cols[::-1] if "æˆäº¤é‡" in c), None)
    call_oi_col  = next((c for c in call_side_cols[::-1] if "æŒä»“é‡" in c), None)
    put_vol_col  = next((c for c in put_side_cols if "æˆäº¤é‡" in c), None)
    put_oi_col   = next((c for c in put_side_cols if "æŒä»“é‡" in c), None)

    out = []
    current_yymm: Optional[str] = None
    current_expiry: Optional[dt.date] = None

    for _, row in df.iterrows():
        vals = ["" if pd.isna(x) else str(x).strip() for x in row.values]

        yymm = _extract_yymm_marker_from_row(vals, prefix)
        if yymm:
            current_yymm = yymm
            current_expiry = infer_expiry(prefix + yymm)
            continue

        if current_yymm is None or current_expiry is None:
            continue

        K = _num(row.get(strike_col))
        if K is None:
            continue

        call_px = _num(row.get(call_price_col))
        put_px  = _num(row.get(put_price_col))

        # poison guard (old bug symptom)
        if call_px is not None and abs(call_px - K) < 1e-12:
            call_px = None
        if put_px is not None and abs(put_px - K) < 1e-12:
            put_px = None

        if call_px is not None and call_px > 0:
            out.append({
                "product": prefix,
                "product_name": OPTIONS_NAME_MAP[prefix],
                "expiry": current_expiry,
                "cp": "C",
                "K": float(K),
                "price": float(call_px),
                "volume": _num(row.get(call_vol_col)) if call_vol_col else None,
                "oi": _num(row.get(call_oi_col)) if call_oi_col else None,
                "symbol": f"{prefix}{current_yymm}-C-{int(K)}",
            })

        if put_px is not None and put_px > 0:
            out.append({
                "product": prefix,
                "product_name": OPTIONS_NAME_MAP[prefix],
                "expiry": current_expiry,
                "cp": "P",
                "K": float(K),
                "price": float(put_px),
                "volume": _num(row.get(put_vol_col)) if put_vol_col else None,
                "oi": _num(row.get(put_oi_col)) if put_oi_col else None,
                "symbol": f"{prefix}{current_yymm}-P-{int(K)}",
            })

    if not out:
        return pd.DataFrame(columns=OPT_COLS)

    return pd.DataFrame(out).sort_values(["expiry", "K", "cp"]).reset_index(drop=True)


def parse_options(df: Optional[pd.DataFrame], prefix: str) -> pd.DataFrame:
    if df is None or df.empty:
        return pd.DataFrame(columns=OPT_COLS)
    return parse_options_paired_cn_mode(df, prefix)


# =========================
# Option math
# =========================
def norm_cdf(x: float) -> float:
    return 0.5 * (1.0 + math.erf(x / math.sqrt(2.0)))


def bs_price_forward(F: float, K: float, T: float, r: float, vol: float, is_call: bool) -> float:
    if T <= 0 or vol <= 0 or F <= 0 or K <= 0:
        return 0.0
    df = math.exp(-r * T)
    sig = vol * math.sqrt(T)
    d1 = (math.log(F / K) + 0.5 * vol * vol * T) / sig
    d2 = d1 - sig
    if is_call:
        return df * (F * norm_cdf(d1) - K * norm_cdf(d2))
    return df * (K * norm_cdf(-d2) - F * norm_cdf(-d1))


def implied_forward_from_parity(C: float, P: float, K: float, T: float, r: float) -> float:
    return K + (C - P) * math.exp(r * T)


def implied_vol(price: float, F: float, K: float, T: float, r: float, is_call: bool) -> Optional[float]:
    if price is None or price <= 0 or T <= 0 or F <= 0 or K <= 0:
        return None

    df = math.exp(-r * T)
    intrinsic = df * max((F - K) if is_call else (K - F), 0.0)
    if price < intrinsic - 1e-8:
        return None

    def f(v):
        return bs_price_forward(F, K, T, r, v, is_call) - price

    try:
        return float(brentq(f, 1e-4, 3.0, maxiter=200))
    except Exception:
        return None


def delta_forward(F: float, K: float, T: float, vol: float, is_call: bool) -> Optional[float]:
    if T <= 0 or vol is None or vol <= 0 or F <= 0 or K <= 0:
        return None
    sig = vol * math.sqrt(T)
    d1 = (math.log(F / K) + 0.5 * vol * vol * T) / sig
    if is_call:
        return norm_cdf(d1)
    return norm_cdf(d1) - 1.0


def rr_bf_25d(ivdf: pd.DataFrame, tol: float = 0.08) -> Optional[Dict[str, float]]:
    d = ivdf.dropna(subset=["iv", "F", "T"]).copy()
    if d.empty:
        return None

    d["is_call"] = d["cp"].eq("C")
    d["delta"] = d.apply(lambda r0: delta_forward(r0["F"], r0["K"], r0["T"], r0["iv"], bool(r0["is_call"])), axis=1)
    d = d.dropna(subset=["delta"])
    if d.empty:
        return None

    calls = d[d["cp"] == "C"].copy()
    puts  = d[d["cp"] == "P"].copy()
    if calls.empty or puts.empty:
        return None

    def nearest(subdf: pd.DataFrame, target: float) -> Optional[float]:
        subdf = subdf.copy()
        subdf["dist"] = (subdf["delta"] - target).abs()
        best = subdf.sort_values("dist").iloc[0]
        if float(best["dist"]) > tol:
            return None
        return float(best["iv"])

    iv_c25 = nearest(calls, 0.25)
    iv_p25 = nearest(puts, -0.25)
    iv_atm = nearest(calls, 0.50)
    if iv_c25 is None or iv_p25 is None or iv_atm is None:
        return None

    rr25 = iv_c25 - iv_p25
    bf25 = 0.5 * (iv_c25 + iv_p25) - iv_atm
    return {"IV_25C": iv_c25, "IV_25P": iv_p25, "IV_ATM": iv_atm, "RR25": rr25, "BF25": bf25}


def compute_robust_forward_from_parity(s: pd.DataFrame, T: float, r: float) -> Tuple[Optional[float], pd.DataFrame, str]:
    calls = s[s["cp"] == "C"][["K", "price"]].rename(columns={"price": "C"})
    puts  = s[s["cp"] == "P"][["K", "price"]].rename(columns={"price": "P"})
    par = pd.merge(calls, puts, on="K", how="inner").dropna()
    if par.empty:
        return None, par, "No C/P pairs"

    par["F_impl"] = par.apply(lambda row: implied_forward_from_parity(row["C"], row["P"], row["K"], T, r), axis=1)
    par = par.dropna(subset=["F_impl"]).sort_values("K")
    if par.empty:
        return None, par, "No valid F_impl"

    par = par[par["F_impl"] > 0].copy()
    if par.empty:
        return None, par, "All F_impl <= 0 (bad prices)"

    med = float(par["F_impl"].median())
    par = par[(par["F_impl"] > 0.5 * med) & (par["F_impl"] < 1.5 * med)].copy()
    if par.empty:
        return None, par, "All filtered as outliers (bad prices)"

    lo = int(len(par) * 0.3)
    hi = int(len(par) * 0.7)
    core = par.iloc[lo:hi] if hi > lo else par
    F = float(core["F_impl"].median())

    note = ""
    if len(core) < 5:
        note = "Too few parity strikes after filtering."
    return F, par, note


# =========================
# ETF Spot Signal Panel (summary at end)
# =========================
def etf_spot_signal_panel(
    *,
    today: dt.date,
    futures_sub: Optional[pd.DataFrame],
    atm_term: Optional[pd.DataFrame],
    skew_term: Optional[pd.DataFrame],
    surf_points: Optional[pd.DataFrame],
    index_spot_px: Optional[float],
    index_spot_ts: Optional[dt.datetime],
    index_spot_source: str,
    r: float,
    q: float = 0.0,
) -> Dict[str, object]:
    """
    Informational ETF spot tilt from derivatives-implied pricing.
    (Other signals unchanged; we only add index spot vs futures basis/carry.)
    """
    score = 0.0
    drivers: List[str] = []
    metrics: Dict[str, Optional[float]] = {}

    metrics["index_spot_px"] = _as_float(index_spot_px)

    if index_spot_px is None:
        drivers.append("æŒ‡æ•°ç°è´§ç¼ºå¤±ï¼šè¯·åœ¨ä¾§è¾¹æ æ‰‹åŠ¨è¾“å…¥ï¼ˆæˆ–ç¨åé‡è¯•Yahooï¼‰ã€‚")
    else:
        ts_str = index_spot_ts.strftime("%Y-%m-%d %H:%M:%S") if index_spot_ts else "unknown-time"
        drivers.append(f"æŒ‡æ•°ç°è´§æ¥æº={index_spot_source}ï¼šSâ‰ˆ{index_spot_px:.2f}ï¼ˆ{ts_str}ï¼‰")

    # ---- Futures curve slope (carry headwind/tailwind proxy)
    curve_slope = None
    if futures_sub is not None and not futures_sub.empty and futures_sub.shape[0] >= 2:
        s = futures_sub.sort_values("expiry")
        f1 = _as_float(s["last"].iloc[0])
        fN = _as_float(s["last"].iloc[-1])
        if f1 and fN and f1 > 0:
            curve_slope = (fN / f1 - 1.0)
    metrics["curve_slope"] = curve_slope

    if curve_slope is not None:
        if curve_slope > 0.002:
            score -= 0.8
            drivers.append(f"æœŸè´§æ›²çº¿ä¸Šå€¾(Contango-ish)ï¼šæ–œç‡â‰ˆ{curve_slope*100:.2f}%ï¼ˆé£é™©åå¼±/carryæ‹–ç´¯ï¼‰")
        elif curve_slope < -0.002:
            score += 0.8
            drivers.append(f"æœŸè´§æ›²çº¿ä¸‹å€¾(Backwardation-ish)ï¼šæ–œç‡â‰ˆ{curve_slope*100:.2f}%ï¼ˆé£é™©åå¼º/carryæ”¯æ’‘ï¼‰")
        else:
            drivers.append(f"æœŸè´§æ›²çº¿æ¥è¿‘å¹³å¦ï¼šæ–œç‡â‰ˆ{curve_slope*100:.2f}%ï¼ˆcarryå½±å“è¾ƒå°ï¼‰")
    else:
        drivers.append("æœŸè´§æ›²çº¿æ–œç‡ä¸å¯å¾—ï¼ˆåˆçº¦ä¸è¶³ï¼‰ã€‚")

    # ---- Spot vs front futures basis/carry (REQUIRES index spot)
    front_F = None
    front_T = None
    if futures_sub is not None and not futures_sub.empty:
        fs = futures_sub.sort_values("expiry")
        front_F = _as_float(fs["last"].iloc[0])
        front_exp = fs["expiry"].iloc[0]
        front_T = yearfrac(today, front_exp)

    metrics["front_fut_px"] = front_F
    basis_pct = None
    carry_excess = None

    if front_F is None or front_T is None or index_spot_px is None:
        drivers.append("è¿‘æœˆæœŸè´§/åˆ°æœŸT/æŒ‡æ•°ç°è´§ç¼ºå¤±ï¼šè·³è¿‡åŸºå·®ä¸carryè®¡ç®—ã€‚")
    elif front_T <= 3 / 365:
        drivers.append("è¿‘æœˆæœŸè´§åˆ°æœŸå¤ªè¿‘ï¼šå¹´åŒ–carryä¼šå¤±çœŸï¼Œå·²è·³è¿‡ã€‚")
    elif not _unit_sanity_ok(front_F, index_spot_px):
        drivers.append("ç°è´§/æœŸè´§å•ä½ç–‘ä¼¼ä¸åŒ¹é…ï¼ˆæ¯”å¦‚ETFå‡€å€¼ vs æŒ‡æ•°ç‚¹ä½ï¼‰ï¼šå·²è·³è¿‡åŸºå·®ä¸carryè®¡ç®—ã€‚")
    else:
        ratio = front_F / index_spot_px
        basis_pct = (ratio - 1.0) * 100.0
        carry_impl = math.log(ratio) / front_T
        carry_excess = (carry_impl - (r - q)) * 100.0
        drivers.append(f"è¿‘æœˆåŸºå·®â‰ˆ{basis_pct:.2f}%ï¼›éšå«å¹´åŒ–carryåç¦»åŸºå‡†(r-q)â‰ˆ{carry_excess:.2f}%/yrï¼ˆq={q:.2%}ï¼‰")

        if carry_excess > 0.50:
            score -= 0.3
            drivers.append("carryæ˜¾è‘—é«˜äºåŸºå‡†ï¼šå¯¹ç°è´§åå‹åŠ›ã€‚")
        elif carry_excess < -0.50:
            score += 0.3
            drivers.append("carryæ˜¾è‘—ä½äºåŸºå‡†ï¼šå¯¹ç°è´§åæ”¯æ’‘ã€‚")
        else:
            drivers.append("Carryæ¥è¿‘åŸºå‡†ï¼šå¯¹æ–¹å‘ä¿¡æ¯å¼±ã€‚")

    metrics["basis_pct"] = basis_pct
    metrics["carry_excess_%yr"] = carry_excess

    # ---- Front-expiry RR25/BF25
    rr25 = bf25 = None
    if skew_term is not None and not skew_term.empty:
        front_e = _pick_front_expiry(skew_term, "expiry")
        if front_e is not None:
            row = skew_term[skew_term["expiry"] == front_e].iloc[0]
            rr25 = _as_float(row.get("RR25"))
            bf25 = _as_float(row.get("BF25"))
    metrics["RR25"] = rr25
    metrics["BF25"] = bf25

    rr_th = 0.005
    bf_th = 0.005

    if rr25 is not None:
        if rr25 <= -rr_th:
            score -= 1.8
            drivers.append(f"RR25ä¸ºè´Ÿï¼ˆputæ›´è´µ/åä¿æŠ¤ï¼‰ï¼šRR25â‰ˆ{rr25*100:.2f} vol pts â†’ é£é™©åè°¨æ…")
        elif rr25 >= rr_th:
            score += 1.8
            drivers.append(f"RR25ä¸ºæ­£ï¼ˆcallæ›´è´µ/åä¸Šè¡Œï¼‰ï¼šRR25â‰ˆ{rr25*100:.2f} vol pts â†’ é£é™©åç§¯æ")
        else:
            drivers.append(f"RR25æ¥è¿‘å¹³ï¼šRR25â‰ˆ{rr25*100:.2f} vol pts")
    else:
        drivers.append("RR25ä¸å¯å¾—ï¼ˆÂ±25Î”é™„è¿‘ç¼ºå°‘å¯ç”¨IVï¼‰ã€‚")

    if bf25 is not None:
        if bf25 >= bf_th:
            score -= 1.0
            drivers.append(f"BF25åé«˜ï¼ˆä¸¤ç¿¼æ›´è´µ/å°¾éƒ¨é£é™©å®šä»·é«˜ï¼‰ï¼šBF25â‰ˆ{bf25*100:.2f} vol pts")
        elif bf25 <= -bf_th:
            score += 0.7
            drivers.append(f"BF25åä½ï¼ˆä¸¤ç¿¼æ›´ä¾¿å®œ/åcarryï¼‰ï¼šBF25â‰ˆ{bf25*100:.2f} vol pts")
        else:
            drivers.append(f"BF25ä¸­æ€§ï¼šBF25â‰ˆ{bf25*100:.2f} vol pts")
    else:
        drivers.append("BF25ä¸å¯å¾—ã€‚")

    # ---- Front-expiry ATM IV (risk regime)
    atm_iv = None
    if atm_term is not None and not atm_term.empty:
        front_e = _pick_front_expiry(atm_term, "expiry")
        if front_e is not None:
            row = atm_term[atm_term["expiry"] == front_e].iloc[0]
            atm_iv = _as_float(row.get("ATM_IV"))
    metrics["ATM_IV"] = atm_iv

    if atm_iv is not None:
        if atm_iv >= 0.25:
            score -= 0.8
            drivers.append(f"è¿‘æœˆATM IVåé«˜ï¼š~{atm_iv*100:.2f}%ï¼ˆæ³¢åŠ¨é¢„æœŸé«˜/åé˜²å®ˆï¼‰")
        elif atm_iv <= 0.15:
            score += 0.5
            drivers.append(f"è¿‘æœˆATM IVåä½ï¼š~{atm_iv*100:.2f}%ï¼ˆç¨³å®š/åé£é™©ï¼‰")
        else:
            drivers.append(f"è¿‘æœˆATM IVä¸­ç­‰ï¼š~{atm_iv*100:.2f}%")
    else:
        drivers.append("ATM IVä¸å¯å¾—ã€‚")

    # ---- Horizon via IV surface concentration
    horizon = _surface_horizon_from_points(surf_points)

    # ---- Bias mapping
    if score >= 1.5:
        bias = "åšå¤š ETF ç°è´§ (LONG / risk-on)"
    elif score <= -1.5:
        bias = "åšç©º/é™ä½ä»“ä½ (SHORT/UNDERWEIGHT / risk-off)"
    else:
        bias = "è§‚æœ›/ä¸­æ€§ (NEUTRAL)"

    confidence = _sigmoid_to_0_100(1.2 * score)
    return {"bias": bias, "horizon": horizon, "confidence": confidence, "score": score, "drivers": drivers, "metrics": metrics}


def render_etf_spot_panel_row(
    *,
    opt_pfx: str,
    opt_cn: str,
    opt_en: str,
    fut_pfx: Optional[str],
    fut_cn: str,
    fut_en: str,
    etf_name_cn: str,
    etf_ticker: str,
    index_name_cn: str,
    index_ticker: str,
    index_spot_px: Optional[float],
    sig: Dict[str, object],
) -> None:
    spot_str = "N/A" if index_spot_px is None else f"{index_spot_px:.2f}"
    st.markdown(
        f"""
        <div class="etf-panel">
          <h3>ğŸ“Œ {etf_name_cn} ({etf_ticker})</h3>
          <div class="subtitle">
            æ ‡çš„æŒ‡æ•°: <b>{index_name_cn}</b> ({index_ticker}) ï½œ æŒ‡æ•°ç°è´§: <b>{spot_str}</b>
            <br/>
            å¯¹åº”æœŸæƒ: <b>{opt_cn}</b> ({opt_pfx}) <span class="smallcap">/ {opt_en}</span>
            ï½œ å¯¹åº”æœŸè´§: {fut_cn} ({fut_pfx or "N/A"}) <span class="smallcap">/ {fut_en if fut_pfx else ""}</span>
          </div>
        </div>
        """,
        unsafe_allow_html=True,
    )

    c1, c2, c3 = st.columns([2.4, 2.0, 1.3])
    with c1:
        st.markdown('<div class="etf-panel kpi-label">æ–¹å‘ / Bias</div>', unsafe_allow_html=True)
        st.markdown(f'<div class="etf-panel kpi">{sig["bias"]}</div>', unsafe_allow_html=True)
    with c2:
        st.markdown('<div class="etf-panel kpi-label">æŒæœ‰å‘¨æœŸ / Horizon</div>', unsafe_allow_html=True)
        st.markdown(f'<div class="etf-panel kpi">{sig["horizon"]}</div>', unsafe_allow_html=True)
    with c3:
        st.markdown('<div class="etf-panel kpi-label">ç½®ä¿¡åº¦ / Confidence</div>', unsafe_allow_html=True)
        st.markdown(f'<div class="etf-panel kpi">{sig["confidence"]}/100</div>', unsafe_allow_html=True)

    with st.expander("é©±åŠ¨å› ç´  & æŒ‡æ ‡ (Drivers & metrics)", expanded=False):
        for d in sig.get("drivers", []):
            st.markdown(f'<div class="etf-panel driver">- {d}</div>', unsafe_allow_html=True)

        m = sig.get("metrics", {})
        st.markdown("<hr/>", unsafe_allow_html=True)
        st.markdown('<div class="etf-panel metricbox"><b>Key metrics:</b></div>', unsafe_allow_html=True)
        st.write(
            {
                "index_spot_px": None if m.get("index_spot_px") is None else round(m["index_spot_px"], 4),
                "front_fut_px": None if m.get("front_fut_px") is None else round(m["front_fut_px"], 4),
                "basis_%": None if m.get("basis_pct") is None else round(m["basis_pct"], 4),
                "carry_excess_%yr": None if m.get("carry_excess_%yr") is None else round(m["carry_excess_%yr"], 4),
                "curve_slope_%": None if m.get("curve_slope") is None else round(100 * m["curve_slope"], 3),
                "RR25_vol_pts": None if m.get("RR25") is None else round(100 * m["RR25"], 3),
                "BF25_vol_pts": None if m.get("BF25") is None else round(100 * m["BF25"], 3),
                "front_ATM_IV_%": None if m.get("ATM_IV") is None else round(100 * m["ATM_IV"], 2),
                "raw_score": round(float(sig.get("score", 0.0)), 3),
            }
        )
        st.markdown('<div class="etf-panel smallcap">æ³¨ï¼šæç¤ºæ€§ä¿¡å·ï¼Œä¸æ„æˆæŠ•èµ„å»ºè®®ã€‚</div>', unsafe_allow_html=True)

    st.markdown("---")


# =========================
# Local-folder file discovery
# =========================
def list_local_files(folder: Path) -> List[Path]:
    exts = (".html", ".htm", ".mhtml")
    files: List[Path] = []
    for ext in exts:
        files.extend(folder.glob(f"*{ext}"))
        files.extend(folder.glob(f"*{ext.upper()}"))
    return sorted(set(files))


def file_text_contains_any(path: Path, needles: List[str], max_bytes: int = 300_000) -> bool:
    try:
        b = path.read_bytes()[:max_bytes]
        s = b.decode(errors="ignore").upper()
        return any(n.upper() in s for n in needles)
    except Exception:
        return False


def split_fut_opt(files: List[Path]) -> Tuple[List[Path], List[Path]]:
    fut_files, opt_files = [], []
    for p in files:
        if file_text_contains_any(p, [r"åˆçº¦åç§°", "IF", "IH", "IM"]) and file_text_contains_any(p, ["IF", "IH", "IM"]):
            fut_files.append(p)
        if file_text_contains_any(p, ["è¡Œæƒä»·", "çœ‹æ¶¨", "çœ‹è·Œ", "IO", "HO", "MO"]):
            opt_files.append(p)
    return fut_files, opt_files


# =========================
# Streamlit UI
# =========================
st.set_page_config(page_title="CFFEX Dashboard", layout="wide")
st.title("CFFEX Futures & Options Dashboard")

# --- Global CSS for ETF signal panel fonts ---
st.markdown(
    """
    <style>
      .etf-panel h3 { margin: 0.25rem 0 0.5rem 0; font-size: 1.05rem; }
      .etf-panel .subtitle { color: #666; font-size: 0.85rem; margin-bottom: 0.5rem; }
      .etf-panel .kpi { font-size: 0.92rem; font-weight: 650; }
      .etf-panel .kpi-label { font-size: 0.78rem; color: #666; }
      .etf-panel .driver { font-size: 0.86rem; line-height: 1.25rem; }
      .etf-panel .metricbox { font-size: 0.82rem; }
      .etf-panel .smallcap { font-size: 0.78rem; color: #777; }
    </style>
    """,
    unsafe_allow_html=True,
)

SCRIPT_DIR = Path(__file__).resolve().parent

with st.sidebar:
    st.header("Inputs")
    today = st.date_input("Valuation date", dt.date.today())
    r = st.number_input("Risk-free rate r (cont.)", value=0.02, step=0.005, format="%.4f")

    q = 0.0
    st.caption("Dividend yield q is assumed 0.00 (zero dividend).")

    folder_str = st.text_input("Data folder (default: script folder)", value=str(SCRIPT_DIR))
    data_dir = Path(folder_str).expanduser().resolve()

    show_debug = st.checkbox("Show debug", value=False)
    tol = st.slider("RR/BF nearest-delta tolerance", min_value=0.02, max_value=0.20, value=0.08, step=0.01)

    st.divider()
    st.subheader("Index Spot (Yahoo + manual override)")
    st.caption("ç”¨äºåŸºå·®/carryï¼šå¿…é¡»è¾“å…¥æŒ‡æ•°ç‚¹ä½ï¼ˆ~3000ï¼‰ï¼Œä¸æ˜¯ETFå‡€å€¼ï¼ˆ~3ï¼‰ã€‚")
    manual_index_spot: Dict[str, Optional[float]] = {}
    for fut_pfx, idx in INDEX_MAP.items():
        key = idx["ticker"]
        val = st.text_input(f"{idx['name_cn']} {key} spot (optional)", value="")
        manual_index_spot[key] = _num(val)

    if st.button("Reload files"):
        st.rerun()


if not data_dir.exists() or not data_dir.is_dir():
    st.error(f"Folder not found: {data_dir}")
    st.stop()

all_files = list_local_files(data_dir)
if not all_files:
    st.warning(f"No .html/.htm/.mhtml files found in: {data_dir}")
    st.stop()

fut_paths, opt_paths = split_fut_opt(all_files)

st.caption(f"Reading files from: {data_dir}")
if show_debug:
    with st.expander("Detected files"):
        st.write("All files:", [p.name for p in all_files])
        st.write("Futures candidates:", [p.name for p in fut_paths])
        st.write("Options candidates:", [p.name for p in opt_paths])

if not fut_paths or not opt_paths:
    st.error("Need BOTH futures and options files in the folder (or your sniff rules didn't match).")
    st.stop()

# ---- Futures
futures_all: List[pd.DataFrame] = []
fut_debug = []

for p in fut_paths:
    tabs = extract_tables_from_mhtml_or_html(p)
    for prefix in FUT_PREFIXES:
        best = pick_best_table(tabs, rf"{prefix}\d{{4}}", want="futures")
        parsed = parse_futures(best, prefix)
        if show_debug:
            fut_debug.append((p.name, prefix, None if best is None else list(best.columns), None if best is None else best.head(8)))
        if not parsed.empty:
            futures_all.append(parsed)

if not futures_all:
    st.error("No futures contracts parsed.")
    if show_debug:
        st.subheader("Debug: Futures picked tables")
        for fname, prefix, cols_preview, head in fut_debug:
            st.markdown(f"**{fname} | {prefix}**")
            st.write("Columns:", cols_preview)
            if head is not None:
                st.dataframe(head, use_container_width=True)
    st.stop()

futures_df = pd.concat(futures_all, ignore_index=True).drop_duplicates()

# ---- Options
options_all: List[pd.DataFrame] = []
opt_debug: List[Dict[str, object]] = []

for p in opt_paths:
    tabs = extract_tables_from_mhtml_or_html(p)
    for prefix in OPT_PREFIXES:
        best = pick_best_table(tabs, rf"{prefix}\d{{4}}", want="options")
        parsed = parse_options(best, prefix)

        if show_debug:
            diag: Dict[str, object] = {"file": p.name, "prefix": prefix, "picked": best is not None}
            if best is not None and not best.empty:
                cols = list(best.columns)
                strike_col = _find_col(cols, "è¡Œæƒä»·")
                if strike_col is not None:
                    k_idx = cols.index(strike_col)
                    call_px_col, put_px_col = pick_adjacent_or_nearest_latest(cols, k_idx)
                else:
                    k_idx, call_px_col, put_px_col = None, None, None
                diag.update({"strike_col": strike_col, "strike_idx": k_idx, "call_px_col": call_px_col, "put_px_col": put_px_col, "cols": cols})
                diag["head20"] = best.head(20)
            opt_debug.append(diag)

        if not parsed.empty:
            options_all.append(parsed)

if not options_all:
    st.error("No options contracts parsed.")
    if show_debug:
        st.subheader("Debug: Options picked tables")
        for d in opt_debug:
            st.markdown(f"**{d.get('file')} | {d.get('prefix')}**")
            st.write({k: d.get(k) for k in ["strike_col", "strike_idx", "call_px_col", "put_px_col"]})
            if d.get("head20") is not None:
                st.dataframe(d["head20"], use_container_width=True)
    st.stop()

options_df = pd.concat(options_all, ignore_index=True).drop_duplicates()

# ---- Sanity: remove price==K poison
options_df = options_df.dropna(subset=["price"]).copy()
options_df = options_df[~(np.abs(options_df["price"] - options_df["K"]) < 1e-12)].copy()
if options_df.empty:
    st.error("All options filtered out by sanity checks (price==K). Parsing still wrong.")
    st.stop()


# =========================
# 1) Futures curve
# =========================
st.subheader("1) Futures term structure")
for pfx in sorted(futures_df["product"].unique()):
    cname = FUTURES_NAME_MAP.get(pfx, pfx)
    sub = futures_df[futures_df["product"] == pfx].sort_values("expiry").copy()
    st.markdown(f"### {cname} ({pfx})")
    st.plotly_chart(px.line(sub, x="expiry", y="last", markers=True, title=f"{cname} â€” Futures curve"), use_container_width=True)
    st.dataframe(sub, use_container_width=True)


# =========================
# 2) Options analytics
# =========================
st.subheader("2) Options analytics (Smile / Forward / Surface / Skew / Carry & Roll)")

signals_summary: List[Dict[str, object]] = []

for pfx in sorted(options_df["product"].unique()):
    cname = OPTIONS_NAME_MAP.get(pfx, pfx)
    st.markdown(f"## {cname} ({pfx})")

    prod = options_df[options_df["product"] == pfx].copy()
    expiries = sorted(prod["expiry"].unique())

    fwd_rows, atm_rows, skew_rows, surf_rows = [], [], [], []

    for expiry in expiries:
        s = prod[prod["expiry"] == expiry].copy()
        T = yearfrac(today, expiry)
        if T <= 0:
            continue

        F, par_dbg, note = compute_robust_forward_from_parity(s[["cp", "K", "price"]].copy(), T, r)
        if F is None:
            if show_debug:
                st.warning(f"{cname} {expiry}: forward not computed ({note})")
            continue

        fwd_rows.append({"expiry": expiry, "T": T, "F": F})
        if note:
            st.warning(f"{cname} {expiry}: {note}")

        s["T"] = T
        s["F"] = F
        s["is_call"] = s["cp"].eq("C")
        s["iv"] = s.apply(lambda row: implied_vol(row["price"], F, row["K"], T, r, bool(row["is_call"])), axis=1)
        ivdf = s.dropna(subset=["iv"]).copy()

        if not ivdf.empty:
            st.plotly_chart(
                px.line(ivdf, x="K", y="iv", color="cp", markers=True,
                        title=f"{cname} â€” IV Smile | Expiry {expiry} | Fâ‰ˆ{F:.2f}"),
                use_container_width=True,
            )

            atm_iv = ivdf[(ivdf["K"] >= 0.98 * F) & (ivdf["K"] <= 1.02 * F)]["iv"].median()
            if pd.notna(atm_iv):
                atm_rows.append({"expiry": expiry, "T": T, "ATM_IV": float(atm_iv)})

            rr = rr_bf_25d(ivdf[["cp", "K", "iv", "F", "T"]], tol=float(tol))
            if rr:
                skew_rows.append({"expiry": expiry, "T": T, "RR25": rr["RR25"], "BF25": rr["BF25"], "IV_ATM_proxy": rr["IV_ATM"]})

            ivdf["m"] = ivdf["K"] / F
            ivdf2 = ivdf[(ivdf["m"] >= 0.8) & (ivdf["m"] <= 1.2)].copy()
            for _, rr0 in ivdf2.iterrows():
                surf_rows.append({"expiry": expiry, "T": T, "m": rr0["m"], "iv": rr0["iv"]})

    fwd_df = pd.DataFrame(fwd_rows, columns=["expiry", "T", "F"])
    if not fwd_df.empty:
        fwd_df = fwd_df.sort_values("expiry")
        st.markdown(f"### {cname} â€” Implied forward curve")
        st.plotly_chart(px.line(fwd_df, x="expiry", y="F", markers=True, title="Implied Forward vs Expiry"), use_container_width=True)
        st.dataframe(fwd_df, use_container_width=True)

        st.markdown(f"### {cname} â€” Forward carry & roll (adjacent expiries)")
        fwd_df2 = fwd_df.sort_values("T").reset_index(drop=True)
        rows = []
        for i in range(len(fwd_df2) - 1):
            e1, e2 = fwd_df2.loc[i, "expiry"], fwd_df2.loc[i + 1, "expiry"]
            T1, T2 = float(fwd_df2.loc[i, "T"]), float(fwd_df2.loc[i + 1, "T"])
            F1, F2 = float(fwd_df2.loc[i, "F"]), float(fwd_df2.loc[i + 1, "F"])
            dT = max(T2 - T1, 1e-9)
            roll_pct = (F2 / F1 - 1.0) * 100.0
            carry_ann = ((F2 / F1) ** (1.0 / dT) - 1.0) * 100.0
            rows.append({"roll_from": str(e1), "roll_to": str(e2), "F_from": F1, "F_to": F2, "Î”T_years": dT,
                         "roll_%": roll_pct, "carry_annualized_%": carry_ann})
        carry_df = pd.DataFrame(rows)
        if not carry_df.empty:
            st.dataframe(carry_df, use_container_width=True)
            st.plotly_chart(px.bar(carry_df, x="roll_to", y="carry_annualized_%", title="Carry (annualized) by next expiry"),
                            use_container_width=True)
    else:
        st.info("Forward curve unavailable (missing valid C/P parity pairs).")

    atm_df = pd.DataFrame(atm_rows, columns=["expiry", "T", "ATM_IV"])
    if not atm_df.empty:
        atm_df = atm_df.sort_values("expiry")
        st.markdown(f"### {cname} â€” ATM IV term structure")
        st.plotly_chart(px.line(atm_df, x="expiry", y="ATM_IV", markers=True, title="ATM IV vs Expiry"), use_container_width=True)
        st.dataframe(atm_df, use_container_width=True)
    else:
        st.info("ATM IV term structure unavailable.")

    skew_df = pd.DataFrame(skew_rows, columns=["expiry", "T", "RR25", "BF25", "IV_ATM_proxy"])
    if not skew_df.empty:
        skew_df = skew_df.sort_values("expiry")
        st.markdown(f"### {cname} â€” Skew term structure (RR25 / BF25)")
        st.plotly_chart(px.line(skew_df, x="expiry", y="RR25", markers=True, title="RR25 vs Expiry"), use_container_width=True)
        st.plotly_chart(px.line(skew_df, x="expiry", y="BF25", markers=True, title="BF25 vs Expiry"), use_container_width=True)
        st.dataframe(skew_df, use_container_width=True)
    else:
        st.info("RR25/BF25 unavailable (insufficient strikes/IV near Â±25d).")

    surf_df = pd.DataFrame(surf_rows, columns=["expiry", "T", "m", "iv"])
    if not surf_df.empty:
        st.markdown(f"### {cname} â€” IV Surface (Expiry Ã— Moneyness K/F)")
        bins = np.linspace(0.8, 1.2, 17)
        surf_df["m_bucket"] = pd.cut(surf_df["m"], bins=bins, include_lowest=True)
        piv = surf_df.groupby(["expiry", "m_bucket"])["iv"].median().reset_index()
        piv["m_mid"] = piv["m_bucket"].apply(lambda x: float((x.left + x.right) / 2.0))
        heat = piv.pivot(index="expiry", columns="m_mid", values="iv").sort_index()
        st.plotly_chart(px.imshow(heat, aspect="auto", title="IV Surface (median IV by moneyness bucket)",
                                  labels={"x": "Moneyness (K/F)", "y": "Expiry", "color": "IV"}),
                        use_container_width=True)
    else:
        st.info("IV surface unavailable (not enough solved IV).")

    # ---- Collect signal for summary at end
    fut_pfx = OPT_TO_FUT.get(pfx)
    fut_sub = futures_df[futures_df["product"] == fut_pfx].sort_values("expiry").copy() if fut_pfx else None

    # Fetch index spot (Yahoo) + manual override fallback
    idx_info = INDEX_MAP.get(fut_pfx) if fut_pfx else None
    idx_ticker = idx_info["ticker"] if idx_info else None

    spot_px, spot_ts, note = (None, None, "no-index")
    spot_source = "none"
    if idx_ticker:
        spot_px, spot_ts, note = fetch_index_spot_price(idx_ticker)
        if spot_px is not None:
            spot_source = "yahoo"
        else:
            # manual override if yahoo failed
            mpx = manual_index_spot.get(idx_ticker)
            if mpx is not None:
                spot_px = float(mpx)
                spot_ts = dt.datetime.now(CN_TZ)
                spot_source = "manual"

    if show_debug and idx_ticker:
        st.caption(f"[debug] index spot {idx_ticker}: {spot_px} | source={spot_source} | note={note}")

    sig = etf_spot_signal_panel(
        today=today,
        futures_sub=fut_sub,
        atm_term=atm_df,
        skew_term=skew_df,
        surf_points=surf_df,
        index_spot_px=spot_px,
        index_spot_ts=spot_ts,
        index_spot_source=spot_source,
        r=float(r),
        q=0.0,
    )

    etf_info = ETF_MAP.get(pfx, {"name_cn": f"{cname} å¯¹åº”ETF", "ticker": "TBD"})
    idx_name_cn = idx_info["name_cn"] if idx_info else "N/A"
    idx_tkr = idx_ticker if idx_ticker else "N/A"

    signals_summary.append(
        {
            "opt_pfx": pfx,
            "opt_cn": OPTIONS_NAME_MAP.get(pfx, pfx),
            "opt_en": OPTIONS_NAME_EN.get(pfx, pfx),
            "fut_pfx": fut_pfx,
            "fut_cn": FUTURES_NAME_MAP.get(fut_pfx, fut_pfx or ""),
            "fut_en": FUTURES_NAME_EN.get(fut_pfx, fut_pfx or ""),
            "etf_name_cn": etf_info.get("name_cn", "ETF"),
            "etf_ticker": etf_info.get("ticker", "TBD"),
            "index_name_cn": idx_name_cn,
            "index_ticker": idx_tkr,
            "index_spot_px": spot_px,
            "sig": sig,
        }
    )


# =========================
# 3) ETF Spot Signal Panel Summary (END)
# =========================
st.subheader("3) ETF Spot Signal Panel")

if not signals_summary:
    st.info("No ETF spot signals available (missing curves / IV / skew inputs).")
else:
    signals_summary = sorted(signals_summary, key=lambda x: x["sig"].get("confidence", 0), reverse=True)

    for item in signals_summary:
        render_etf_spot_panel_row(
            opt_pfx=item["opt_pfx"],
            opt_cn=item["opt_cn"],
            opt_en=item["opt_en"],
            fut_pfx=item["fut_pfx"],
            fut_cn=item["fut_cn"],
            fut_en=item["fut_en"],
            etf_name_cn=item["etf_name_cn"],
            etf_ticker=item["etf_ticker"],
            index_name_cn=item["index_name_cn"],
            index_ticker=item["index_ticker"],
            index_spot_px=item["index_spot_px"],
            sig=item["sig"],
        )


if show_debug:
    st.subheader("Debug")
    st.write("Options parsed preview (first 200 rows)")
    st.dataframe(options_df.head(200), use_container_width=True)
    st.write("Unique expiries by product:")
    st.write(options_df.groupby("product")["expiry"].nunique())
    st.write(options_df.groupby("product")["expiry"].unique())
